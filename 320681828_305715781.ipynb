{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_DSONP2cR58a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b09446bd-3424-4401-ecb6-4a42e6b8f133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ReliefF in /usr/local/lib/python3.7/dist-packages (0.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ReliefF) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from ReliefF) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from ReliefF) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ReliefF) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->ReliefF) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sklearn_relief in /usr/local/lib/python3.7/dist-packages (1.0.0b2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from sklearn_relief) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from sklearn_relief) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from sklearn_relief) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->sklearn_relief) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->sklearn_relief) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mrmr_selection in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from mrmr_selection) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mrmr_selection) (4.64.0)\n",
            "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from mrmr_selection) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from mrmr_selection) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.7/dist-packages (from mrmr_selection) (1.21.6)\n",
            "Requirement already satisfied: category-encoders in /usr/local/lib/python3.7/dist-packages (from mrmr_selection) (2.5.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from mrmr_selection) (0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mrmr_selection) (2.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.3->mrmr_selection) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.3->mrmr_selection) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.3->mrmr_selection) (1.15.0)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category-encoders->mrmr_selection) (0.10.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders->mrmr_selection) (0.5.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category-encoders->mrmr_selection) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category-encoders->mrmr_selection) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mrmr_selection) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install ReliefF \n",
        "!pip install sklearn_relief\n",
        "!pip install mrmr_selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq7prrc2c3kd"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HoLK32MFJPPa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import plot_importance\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "import sklearn_relief as sr\n",
        "from scipy.io import arff\n",
        "from functools import reduce\n",
        "from mrmr import mrmr_classif\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vtLYZjm-KV7g"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
        "from sklearn.feature_selection import SelectKBest,SelectPercentile,f_classif ,r_regression,SelectFdr,RFE\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.svm import SVR\n",
        "from sklearn import metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QU8NJnZuJUBm"
      },
      "outputs": [],
      "source": [
        "filePath = \"./SPECTF.train.csv\"\n",
        "n_featurs_selected = 25\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1xNoOkZoEOu",
        "outputId": "d1010d9c-a55a-4408-e233-5a3f09614109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(80, 45)\n",
            "(80, 44)\n",
            "(80,)\n",
            "Int64Index([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
            "            18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
            "            35, 36, 37, 38, 39, 40, 41, 42, 43, 44],\n",
            "           dtype='int64')\n"
          ]
        }
      ],
      "source": [
        "X = pd.read_csv(filePath,header = None)\n",
        "df = pd.read_csv(filePath,header = None)\n",
        "\n",
        "#clase = the column we would like to classify\n",
        "y = X.iloc[:,0]\n",
        "print(X.shape)\n",
        "\n",
        "#drops = Array of columns drop\n",
        "X.drop( columns = X.columns[0], axis=1, inplace=True)\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "print(X.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C96VbnA0xS-r"
      },
      "source": [
        "The correletion of featurs to the the target by Information-Gain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gqr-RPT-KGGe"
      },
      "outputs": [],
      "source": [
        "#information gain\n",
        "def IF_selection(X,y,n_featurs_selected):\n",
        "  selector = SelectKBest(mutual_info_classif,k = n_featurs_selected )\n",
        "  X_reduced = selector.fit_transform(X,y)\n",
        "  cols = selector.get_support(indices = True)\n",
        "  selected_columns = X.iloc[:,cols].columns.tolist()\n",
        "  # mi = mutual_info_classif(X, y)\n",
        "  # mi = pd.Series(mi)\n",
        "  # mi.index = X.columns\n",
        "  # mi_sort = mi.sort_values(ascending=False)\n",
        "  # best_k =  mi_sort[:n_featurs_selected,]\n",
        "  # mi.sort_values(ascending=True).plot.bar(figsize=(10, 4))\n",
        "  # IF_k_best = []\n",
        "  # for x in best_k.items():\n",
        "  #   IF_k_best.append(x[0])\n",
        "  # IF_k_best = np.array(IF_k_best)\n",
        "  return selected_columns\n",
        "# IF_k_best = IF_selection(X,y,n_featurs_selected)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3SKVLkHt4u3"
      },
      "source": [
        "Correlation-based Feature Selection, CFS: This is a simple filter algorithm that ranks feature subsets according to a correlation-based heuristic evaluation function [17]. The bias of the evaluation function is toward subsets that contain features that are highly correlated with the class and uncorrelated with each other. Irrelevant features should be ignored and redundant features should be screened out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aU6ZaHPcVvVr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "from mutual_information import su_calculation\n",
        "\n",
        "#need to load the folder CFSmethod from the git below\n",
        "#https://github.com/ZixiaoShen/Correlation-based-Feature-Selection/blob/master/CFSmethod/CFS.py\n",
        "def merit_calculation(X, y):\n",
        "    \"\"\"\n",
        "    This function calculates the merit of X given class labels y, where\n",
        "    merits = (k * rcf) / sqrt (k + k*(k-1)*rff)\n",
        "    rcf = (1/k)*sum(su(fi, y)) for all fi in X\n",
        "    rff = (1/(k*(k-1)))*sum(su(fi, fj)) for all fi and fj in X\n",
        "    :param X:  {numpy array}, shape (n_samples, n_features) input data\n",
        "    :param y:  {numpy array}, shape (n_samples) input class labels\n",
        "    :return merits: {float}  merit of a feature subset X\n",
        "    \"\"\"\n",
        "\n",
        "    n_samples, n_features = X.shape\n",
        "    rff = 0\n",
        "    rcf = 0\n",
        "    for i in range(n_features):\n",
        "        fi = X[:, i]\n",
        "        rcf += su_calculation(fi, y)  # su is the symmetrical uncertainty of fi and y\n",
        "        for j in range(n_features):\n",
        "            if j > i:\n",
        "                fj = X[:, j]\n",
        "                rff += su_calculation(fi, fj)\n",
        "    rff *= 2\n",
        "    merits = rcf / np.sqrt(n_features + rff)\n",
        "    return merits\n",
        "\n",
        "\n",
        "def cfs(X, y):\n",
        "    \"\"\"\n",
        "    This function uses a correlation based heuristic to evaluate the worth of features which is called CFS\n",
        "    :param X: {numpy array}, shape (n_samples, n_features) input data\n",
        "    :param y: {numpy array}, shape (n_samples) input class labels\n",
        "    :return F: {numpy array}, index of selected features\n",
        "    \"\"\"\n",
        "\n",
        "    n_samples, n_features = X.shape\n",
        "    F = []\n",
        "    M = []  # M stores the merit values\n",
        "    while True:\n",
        "        merit = -100000000000\n",
        "        idx = -1\n",
        "        for i in range(n_features):\n",
        "            if i not in F:\n",
        "                F.append(i)\n",
        "                # calculate the merit of current selected features\n",
        "                t = merit_calculation(X[:, F], y)\n",
        "                if t > merit:\n",
        "                    merit = t\n",
        "                    idx = i\n",
        "                F.pop()\n",
        "        F.append(idx)\n",
        "        M.append(merit)\n",
        "        if len(M) > 5:\n",
        "            if M[len(M)-1] <=M[len(M)-2]:\n",
        "                if M[len(M)-2] <= M[len(M)-3]:\n",
        "                    if M[len(M)-3] <= M[len(M)-4]:\n",
        "                        if M[len(M)-4] <= M[len(M)-5]:\n",
        "                            break\n",
        "    return np.array(F)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6uStgUWqehit"
      },
      "outputs": [],
      "source": [
        "def cfs_selection(X,y,k):\n",
        "# x - data set witout label class\n",
        "# y - label class \n",
        "# return vector of best featurs\n",
        "  cfs_vectore = cfs(X.to_numpy(),y.to_numpy())\n",
        "  return cfs_vectore[:k]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VKc1Dh0t-j-"
      },
      "source": [
        "Consistency-based Filter: The consistency-based filter evaluates the worth of a subset of features by the level of consistency in the class values when the training instances are projected onto the subset of attributes. The algorithm generates a random subset S from the number of features in every round. If the number of features of S is less than the current best, the data with the features prescribed in S is checked against the inconsistency criterion. If its inconsistency rate is below a pre-specified one, S becomes the new current best.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aNlYaDOsfkou"
      },
      "outputs": [],
      "source": [
        "def consistency_feature_selection(X,y,k):\n",
        "# x - data set witout label class\n",
        "# y - label class \n",
        "# return vector of best featurs\n",
        "\n",
        "  # K_best = SelectPercentile().fit(X,y)\n",
        "  # #Consistency-based Filter: The consistency-based filter\n",
        "  # # Get columns to keep and create new dataframe with those only\n",
        "  # cols = K_best.get_support(indices=True)\n",
        "  # features_df_new = X.iloc[:,cols]\n",
        "  # features_df_new.columns\n",
        "  # const_k_best = features_df_new.columns.values\n",
        "  estimator = SVR(kernel=\"linear\")\n",
        "  selector = RFE(estimator, n_features_to_select=k, step=1)\n",
        "  selector = selector.fit(X, y)\n",
        "  names = X.columns[selector.get_support()]\n",
        "  return names\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3qiEEHYuEWY"
      },
      "source": [
        "INTERACT: The INTERACT algorithm  is based on symmetrical uncertainty (SU). Besides SU, INTERACT also\n",
        "includes the consistency contribution (c-contribution). The\n",
        "algorithm consists of ranking the features in descending order\n",
        "based on their SU values and then evaluating one by one\n",
        "starting from the end of the ranked feature list. Then, a feature\n",
        "is selected or not depending on a threshold, either established\n",
        "by the user or employing the default value provided. Theoretically, INTERACT can handle feature interaction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_YXU2aNGuAcM"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def interact(X,y,K):\n",
        "  knn = KNeighborsClassifier(n_neighbors=3)\n",
        "  sfs = SequentialFeatureSelector(knn, n_features_to_select=K)\n",
        "  sfs.fit(X, y)\n",
        "  SequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3),\n",
        "                            n_features_to_select=K)\n",
        "  sfs.get_support()\n",
        "  cols = sfs.get_support(indices = True)\n",
        "  selected_columns = X.iloc[:,cols].columns.tolist() \n",
        "  return selected_columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eY5DW69t_vP"
      },
      "source": [
        "Correlation-based Feature Selection \n",
        "Selecting highly correlated features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "bF3fFGPUri0I"
      },
      "outputs": [],
      "source": [
        "#Correlation-based Feature Selection\n",
        "def cor_k_best(df,colum_target,min_cor):\n",
        "# input :  df - data frame\n",
        "# colum_target - index colum of target class\n",
        "# min_cor - minmum corllation\n",
        "# return : vecotr of best feature\n",
        "  cor = df.corr()\n",
        "  cor_target = abs(cor[colum_target])\n",
        "\n",
        "  relevant_features = cor_target[cor_target>min_cor]\n",
        "  cor_k_best = []\n",
        "  for x in relevant_features.items():\n",
        "    if(x[0] == colum_target ):\n",
        "      continue\n",
        "    cor_k_best.append(x[0])\n",
        "  cor_k_best_feature = np.array(cor_k_best)\n",
        "\n",
        "  return cor_k_best_feature\n",
        "\n",
        "# cor_k_best_feature = cor_k_best(df,0,0.3)\n",
        "# cor_k_best_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6HMxLocQgbp"
      },
      "source": [
        "ReliefF  is an extension of the original Relief algorithm  that adds the ability of dealing with multiclass problems and it is more robust and capable of dealing with incomplete and noisy data. The Relief family of methods are specially attractive because they may be applied in all situations, have low bias, include interaction among features and may capture local dependencies that other methods miss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iuhqRq55J-Bo"
      },
      "outputs": [],
      "source": [
        "import sklearn_relief as relief\n",
        "\n",
        "def relif_k_best(X,y,n_featurs_selected):\n",
        "# \n",
        "# x - data set witout label class\n",
        "# y - label class \n",
        "# n_featurs_selected - number of k best featurs\n",
        "# return vector size of n_featurs_selected best features\n",
        "# \n",
        "  my_input_matrix = np.array(X)\n",
        "  my_label_vector = np.array(y)\n",
        "  r = relief.Relief(\n",
        "      n_features=X.shape[1] # Choose the best K features\n",
        "  ) # Will run by default on all processors concurrently\n",
        "\n",
        "  my_transformed_matrix = r.fit_transform(\n",
        "      my_input_matrix,\n",
        "      my_label_vector\n",
        "  )\n",
        "  dp_rate_index = pd.DataFrame(r.w_, index = None)\n",
        "  dp_rate_index.head()\n",
        "  dp_rate_index_sort = dp_rate_index.sort_values([0],axis = 0, ascending= False)\n",
        "  relief_k_best = []\n",
        "  scores = []\n",
        "  for x in dp_rate_index_sort.iterrows():\n",
        "    relief_k_best.append(x[0])\n",
        "    scores.append(x[1][0])\n",
        "  relief_k_best = relief_k_best[:n_featurs_selected]\n",
        "  scores = scores[:n_featurs_selected]\n",
        "  return relief_k_best,scores\n",
        "\n",
        "relief_k_best, scores = relif_k_best(X,y,n_featurs_selected)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IkRDqNDRDvI"
      },
      "source": [
        "all the featur that selected from all our 5 five differents feature selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8XmintZWdRy"
      },
      "source": [
        "The complete selection algo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fmVvBqgwWlOR"
      },
      "outputs": [],
      "source": [
        "def Ensemble_selection(data,min_cor,colum_target,K = 25):\n",
        "  #return vector of 0,1 for selecting index of fetur colum\n",
        "\n",
        "  y = data.iloc[:,colum_target]\n",
        "  X = data.drop( columns = data.columns[colum_target], axis=1, inplace=False)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "  IF_k_best = IF_selection(X,y,K)\n",
        "  IF_k_best = [ data.columns.get_loc(x) for x in IF_k_best ]\n",
        "  cfs_vectore = cfs_selection(X,y,K)\n",
        "\n",
        "  const_k_best = consistency_feature_selection(X,y,K)\n",
        "  const_k_best = [ data.columns.get_loc(x) for x in const_k_best ]\n",
        "\n",
        "  # interact_k_best = interact(X,y,n_featurs_selected)\n",
        "  # print(interact_k_best)\n",
        "  # cor_k_best_featur = cor_k_best(data,colum_target,min_cor)\n",
        "  # print(cor_k_best_featur)\n",
        "\n",
        "  relief_k_best = relif_k_best(X,y,K)\n",
        "  relief_k_best = relief_k_best[0]\n",
        "  feature_selection = []\n",
        "  feature_selection =reduce(np.union1d, (relief_k_best,const_k_best,cfs_vectore,IF_k_best))\n",
        "\n",
        "  zeros = np.zeros((X.shape[1]+1,), dtype=int)\n",
        "  # for x in np.nditer(feature_selection):\n",
        "  #   zeros[x] = 1\n",
        "  return zeros,feature_selection\n",
        "\n",
        "# featurs = Ensemble_selection(df,0.3,0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3J75_IMXYiiO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cefd03ce-a60f-4b89-e79e-27409cde5153"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the number in the vectors represent the featur index in the dataset \n",
            "Index with 1 mean selected feature: \n",
            "[ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 16 17 19 21 22 23 24 25 26 27\n",
            " 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44]\n",
            " \n",
            "Index with 1 mean selected feature: \n",
            "[ 0  1  2  3  4  5  7  8  9 10 11 12 14 16 17 18 19 21 22 23 24 25 26 27\n",
            " 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44]\n",
            " \n",
            "Index with 1 mean selected feature: \n",
            "[ 0  1  2  3  4  5  7  8  9 10 11 12 13 14 16 17 19 21 22 23 24 25 26 27\n",
            " 28 29 30 31 32 33 34 35 37 38 39 40 41 42 43 44]\n",
            " \n"
          ]
        }
      ],
      "source": [
        "featurs = []\n",
        "print(\"the number in the vectors represent the featur index in the dataset \")\n",
        "for i in range(3):\n",
        "  featurs.append(Ensemble_selection(df,0.3,0)[1])\n",
        "  print(\"Index with 1 mean selected feature: \")\n",
        "  print(featurs[i])\n",
        "  print(\" \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wUm0xZhqTyz8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "df.to_csv('FINAL.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Izfz5FJorEcd"
      },
      "outputs": [],
      "source": [
        "# file1 file2 from bioconductor folder\n",
        "#\n",
        "file1 = './CLL.csv'\n",
        "file2 = './ayeastCC.csv'\n",
        " \n",
        "file1_pd = pd.read_csv(file1,header = None, index_col = 0)\n",
        "file1_pd = file1_pd.T\n",
        "file1_pd.drop(columns=file1_pd.columns[0], axis=1, inplace=True)\n",
        "first_column = file1_pd.pop('Class')\n",
        "file1_pd.insert(file1_pd.shape[1], 'Class', first_column)\n",
        "\n",
        "file2_pd = pd.read_csv(file2,header = None, index_col = 0)\n",
        "file2_pd = file2_pd.T\n",
        "file2_pd.drop(columns=file2_pd.columns[0], axis=1, inplace=True)\n",
        "first_column = file2_pd.pop('Class')\n",
        "file2_pd.insert(file2_pd.shape[1], 'Class', first_column)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "B4XMS0LexL-b"
      },
      "outputs": [],
      "source": [
        "# from folder ARFF got Lymphoma.arff and Breast.arff\n",
        "data_breast = arff.loadarff(\"./Breast.arff\")\n",
        "Breast_df = pd.DataFrame(data_breast[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jJHt5in2pEW",
        "outputId": "6e01c9f5-5ea5-48d4-a8c7-8c359649d4ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(253, 15155)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "Ovarian = arff.loadarff(\"./Ovarian.arff\")\n",
        "Ovarian = pd.DataFrame(Ovarian[0])\n",
        "Ovarian.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VXhtzaH-Oxf"
      },
      "source": [
        "Check for NA in taget class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ajDXUobgynlx"
      },
      "outputs": [],
      "source": [
        "df_arr  = [file1_pd,file2_pd,Breast_df,Ovarian]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUNASiufyKcx"
      },
      "source": [
        "Preprocessing the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4va3pkzSyGnX"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "This function is responsible for reading the data from the csv file, data Pre-processin,\n",
        "Converting Categorical Variables into numerical, splitting the data into train and test sets and returning them\n",
        "fitting the modle and returning its hyper parameters\n",
        "'''\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "import scipy\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def classify2(df):\n",
        "  # categorial missing values fill with the most common one\n",
        "  # numerical misssing values fill with the mean.\n",
        "  # cf = scipy.sparse.csr_matrix(df.values)\n",
        "  # selector = VarianceThreshold()\n",
        "  # selector.fit_transform(cf)\n",
        "  # df = pd.DataFrame.sparse.from_spmatrix(cf)\n",
        "\n",
        "  for col in df :\n",
        "    if df[col].dtype == np.object: \n",
        "      df[col].fillna(df[col].mode()[0], inplace = True)\n",
        "    else :\n",
        "      df[col].fillna(df[col].mean(),inplace = True)\n",
        "\n",
        "  #Converting Categorical Variables into numerical\n",
        "  label_encoder = preprocessing.LabelEncoder()\n",
        "  for col in df.columns:\n",
        "    if df[col].dtype == np.object:\n",
        "      df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "  #clase = the column we would like to classify\n",
        "\n",
        "  \n",
        "  return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "P1EXJr7-1tgX"
      },
      "outputs": [],
      "source": [
        "# X_y_arr = [(X,y of file 1), (X,y of file2) , (X,y of file 3), (X,y of file 4)]\n",
        "X_y_arr = [ (classify2(x)) for x in df_arr]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "FJqVuS4GE82X"
      },
      "outputs": [],
      "source": [
        "# selected_features = mrmr_classif(X=X, y=y, K=10, return_scores=True)\n",
        "def mrmr_scores_names(selected_features):\n",
        "  scores = []\n",
        "  feature_name = []\n",
        "  for x in selected_features[1][selected_features[0]].items():\n",
        "    scores.append(x[1]) #scors\n",
        "    feature_name.append(x[0]) # return names\n",
        "    index_col =X.columns.get_loc(x[0]) #col index\n",
        "  return feature_name,scores,index_col\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD6xw_0eN4nV"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.metrics import average_precision_score, precision_recall_curve , auc\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "import time\n",
        "\n",
        "K = [1,2,3,4,5,10,15,20,25,30,50,100]\n",
        "models = [('LR',LogisticRegression()), ('RF',RandomForestClassifier()),('Naive_Bayes',GaussianNB()),('KNN',KNeighborsClassifier(n_neighbors=3))]\n",
        "ft_selection = [('FDR',SelectFdr(alpha = 0.1)), ('RFE', RFE(SVC())), ('Relief', relif_k_best),('mRMR',mrmr_classif),(\"MyAlgo-Ensamble\",Ensemble_selection)]\n",
        "datasets_name_mapper = {0 : 'CLL', 1 : 'ayeastCC', 2 : 'Breast', 3 : 'Ovarian'}\n",
        "cv_type = {50 : 'LeavePairOut', 100 : 'LeaveOneOut', 200: 'Fold CV10', 1000 : 'Folds CV5'}\n",
        "results = { 'Dataset Name': [],'Number of samples': [],'Original Number of features' : [], 'Filtering Algorithm': [],'Learning algorithm': [],\n",
        "           'Number of features selected':[], 'CV Method & FoldNumer' : [], 'AUC': [], 'ACC':[] ,'List of Selected Features Names (Long STRING)':[],'Selected Features scores': [],'Selection_time_sec': [], 'Fit_time_sec': [],'Inference time':[]}\n",
        "\n",
        "for idx, dataset in enumerate(X_y_arr[1:]):\n",
        "  print(datasets_name_mapper[idx])\n",
        "  for name, model in models:\n",
        "    print(name,model)\n",
        "    for ft, ft_select in ft_selection:\n",
        "      print(ft)\n",
        "      for k in K:\n",
        "        print(k)\n",
        "        X, y = dataset.drop('Class', axis = 1), dataset['Class']\n",
        "        origin_N_featurs = X.shape[1]\n",
        "        if ft == 'RFE':\n",
        "          estimator = SVR(kernel=\"linear\")\n",
        "          selector = RFE(estimator, n_features_to_select=k, step=1)\n",
        "          start = time.time()\n",
        "          new_X = selector.fit_transform(X, y)\n",
        "          stop = time.time()\n",
        "          time_select = stop - start\n",
        "          scores = []\n",
        "          names = []\n",
        "          for x in selector.get_feature_names_out():\n",
        "            name = X.columns.get_loc(x)\n",
        "            scores.append(selector.ranking_[name])\n",
        "            names.append(x)\n",
        "        elif ft == 'Relief':\n",
        "          cols,scores = ft_select(X, y, k)\n",
        "          names = X.columns[cols]\n",
        "          new_X = X.iloc[:,cols]\n",
        "        elif ft == 'mRMR':\n",
        "          cols = mrmr_classif(X=X, y=y, K=k,return_scores=True)\n",
        "          names,scores,cols_index = mrmr_scores_names(cols)\n",
        "          new_X = X.iloc[:,cols_index]\n",
        "        elif ft == 'MyAlgo-Ensamble':\n",
        "          start = time.time()\n",
        "          t = dataset.columns.get_loc('Class')\n",
        "          featurs = Ensemble_selection(dataset,0.3,t,k)[1]\n",
        "          stop = time.time()\n",
        "          time_select = stop - start\n",
        "          names = featurs\n",
        "          scores = []\n",
        "         \n",
        "        else:\n",
        "          start = time.time()\n",
        "          new_X = ft_select.fit_transform(X, y)\n",
        "          stop = time.time()\n",
        "          time_select = stop - start\n",
        "          names = []\n",
        "          scores = []\n",
        "        N = new_X.shape[0]\n",
        "        if N < 50:\n",
        "          cv = N-2\n",
        "          t = 50\n",
        "        if N >= 50 and  N <= 100:\n",
        "          cv = N-1\n",
        "          t = 100\n",
        "        if N >100 and N < 1000 :\n",
        "          cv = 10\n",
        "          t= 200\n",
        "        if N > 1000:\n",
        "          cv = 5\n",
        "          t = 1000\n",
        "        try:\n",
        "          res = cross_validate(model,new_X, y, scoring=('accuracy', 'roc_auc','recall'), cv = cv)\n",
        "        except:\n",
        "          res = cross_validate(model,new_X, y, scoring=('accuracy', 'roc_auc','recall'), cv = 10)\n",
        "        stop = time.time()\n",
        "        fit_model_time = stop - start\n",
        "        results['Dataset Name'].append(datasets_name_mapper[idx])\n",
        "        results['Number of samples'].append(N)\n",
        "        results['Original Number of features'].append(origin_N_featurs)\n",
        "        results['Filtering Algorithm'].append(ft)\n",
        "        results['Learning algorithm'].append(name)\n",
        "        results['Number of features selected'].append(k)\n",
        "        results['CV Method & FoldNumer'].append(cv_type[t])\n",
        "        results['AUC'].append(res['test_roc_auc'].mean())\n",
        "        results['ACC'].append(res['test_accuracy'].mean())\n",
        "        results['List of Selected Features Names (Long STRING)'].append(names)\n",
        "        results['Selected Features scores'].append(scores)\n",
        "        results['Inference time'].append(fit_model_time)\n",
        "        results['Selection_time_sec'].append(time_select)\n",
        "        results['Fit_time_sec'].append(fit_model_time)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3mWBdE94lVw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "new_Df = pd.DataFrame(results)\n",
        "new_Df.to_csv('./result')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1rn_YhXbwNE"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "sklearn.metrics.SCORERS.keys()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Ass4.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}